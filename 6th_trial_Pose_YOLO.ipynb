{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO4BLaXX32wPhyqDzeBwXQJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_kl-A7dtF3l",
        "outputId": "e5761035-1c69-40f4-9a7f-ebc51f5a7393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.28-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.28-py3-none-any.whl (881 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.2/881.2 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.11-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.28 ultralytics-thop-2.0.11\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install ultralytics\n",
        "!pip install opencv-python\n",
        "\n",
        "# Import necessary libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pose Estimation"
      ],
      "metadata": {
        "id": "hlcc4vjRMcQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "yolo_model = YOLO(\"yolo11n-pose.pt\")\n",
        "\n",
        "# Activity\n",
        "activity_videos = {\n",
        "    'sitting': ['/content/drive/MyDrive/P2/DATA/Sitting/001.mp4',\n",
        "                '/content/drive/MyDrive/P2/DATA/Sitting/002.mp4',\n",
        "                #'/content/drive/MyDrive/P2/DATA/Sitting/003.mp4',\n",
        "                #'/content/drive/MyDrive/P2/DATA/Sitting/004.mp4',\n",
        "                #'/content/drive/MyDrive/P2/DATA/Sitting/005.mp4'\n",
        "                ],\n",
        "    'laying': ['/content/drive/MyDrive/P2/DATA/Laying/001.mp4',\n",
        "               '/content/drive/MyDrive/P2/DATA/Laying/002.mp4',\n",
        "               #'/content/drive/MyDrive/P2/DATA/Laying/003.mp4',\n",
        "               #'/content/drive/MyDrive/P2/DATA/Laying/004.mp4',\n",
        "               #'/content/drive/MyDrive/P2/DATA/Laying/005.mp4'\n",
        "               ],\n",
        "    'jumping': ['/content/drive/MyDrive/P2/DATA/Jumping/001.mp4',\n",
        "                '/content/drive/MyDrive/P2/DATA/Jumping/002.mp4',\n",
        "                #'/content/drive/MyDrive/P2/DATA/Jumping/003.mp4',\n",
        "                #'/content/drive/MyDrive/P2/DATA/Jumping/004.mp4',\n",
        "                #'/content/drive/MyDrive/P2/DATA/Jumping/005.mp4'\n",
        "                ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX6FN0kpvjWZ",
        "outputId": "beff8b93-1c3f-4b79-8526-84ff27c6cd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt to 'yolo11n-pose.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.97M/5.97M [00:00<00:00, 378MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store keypoints data for all activities\n",
        "all_activity_points = []\n",
        "\n",
        "# Loop\n",
        "for activity, video_paths in activity_videos.items():\n",
        "    for video_path in video_paths:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        seconds = round(frames / fps)\n",
        "\n",
        "        frame_total = 30  # Number of frames to process\n",
        "        i, a = 0, 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            cap.set(cv2.CAP_PROP_POS_MSEC, (i * ((seconds / frame_total) * 1000)))\n",
        "            flag, frame = cap.read()\n",
        "\n",
        "            if not flag:\n",
        "                break\n",
        "\n",
        "            # Perform YOLO pose estimation on the frame\n",
        "            results = yolo_model(frame, verbose=False)\n",
        "\n",
        "            for r in results:\n",
        "                bound_box = r.boxes.xyxy\n",
        "                conf = r.boxes.conf.tolist()\n",
        "                keypoints = r.keypoints.xyn.tolist()\n",
        "\n",
        "                for index, box in enumerate(bound_box):\n",
        "                    if conf[index] > 0.75:\n",
        "                        x1, y1, x2, y2 = box.tolist()\n",
        "                        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "                        label = f'{activity}'\n",
        "                        cv2.putText(frame, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "                        skeleton_connections = [(0, 1), (1, 2), (2, 3), (3, 4),  # Left arm\n",
        "                                                (0, 5), (5, 6), (6, 7), (7, 8),  # Right arm\n",
        "                                                (9, 10), (10, 11), (11, 12),     # Left leg\n",
        "                                                (9, 13), (13, 14), (14, 15),     # Right leg\n",
        "                                                (0, 9)]                          # Spine\n",
        "                        for p1, p2 in skeleton_connections:\n",
        "                            x1, y1 = keypoints[index][p1]\n",
        "                            x2, y2 = keypoints[index][p2]\n",
        "                            cv2.line(frame, (int(x1 * frame.shape[1]), int(y1 * frame.shape[0])),\n",
        "                                     (int(x2 * frame.shape[1]), int(y2 * frame.shape[0])), (0, 0, 255), 2)\n",
        "\n",
        "                        output_path = f'/content/drive/MyDrive/P2/YOLO/output/{activity}_out_{a}_skeleton.jpg'\n",
        "                        cv2.imwrite(output_path, frame)\n",
        "\n",
        "                        data = {'image_name': f'{activity}_person_{a}.jpg', 'activity': activity}\n",
        "                        for j in range(len(keypoints[index])):\n",
        "                            data[f'x{j}'] = keypoints[index][j][0]\n",
        "                            data[f'y{j}'] = keypoints[index][j][1]\n",
        "\n",
        "                        all_activity_points.append(data)\n",
        "                        a += 1\n",
        "            i += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "# Save keypoints data into a CSV\n",
        "df = pd.DataFrame(all_activity_points)\n",
        "csv_file_path = '/content/drive/MyDrive/P2/DATA/activity_keypoints_01.csv'\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "print(f\"Keypoints data saved to {csv_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o4CKagZwQfS",
        "outputId": "765a93b8-813f-4460-84c0-1750cd479020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keypoints data saved to /content/drive/MyDrive/P2/DATA/activity_keypoints_01.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "_uR2iVSMMewJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "### Change this part!!\n",
        "df = pd.read_csv(csv_file_path)\n",
        "X = df.drop(['image_name', 'activity'], axis=1)\n",
        "y = df['activity'].astype('category').cat.codes\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=32)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "FK1f7pT0wWvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activity Classification"
      ],
      "metadata": {
        "id": "zotooEr8MhH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
        "\n",
        "# Modle Compile and train\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train_cat, epochs=20, batch_size=32, validation_data=(X_test, y_test_cat))\n",
        "\n",
        "# Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DatFl5vEMaRl",
        "outputId": "889b044f-f2a7-4c20-ada5-205af75df0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.2777 - loss: 1.1597 - val_accuracy: 0.0962 - val_loss: 1.1168\n",
            "Epoch 2/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1308 - loss: 1.1091 - val_accuracy: 0.1538 - val_loss: 1.0976\n",
            "Epoch 3/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2821 - loss: 1.0782 - val_accuracy: 0.2308 - val_loss: 1.0807\n",
            "Epoch 4/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4538 - loss: 1.0523 - val_accuracy: 0.5385 - val_loss: 1.0654\n",
            "Epoch 5/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6840 - loss: 1.0166 - val_accuracy: 0.5577 - val_loss: 1.0491\n",
            "Epoch 6/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6971 - loss: 0.9993 - val_accuracy: 0.5769 - val_loss: 1.0279\n",
            "Epoch 7/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6852 - loss: 0.9745 - val_accuracy: 0.6346 - val_loss: 1.0009\n",
            "Epoch 8/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7921 - loss: 0.9661 - val_accuracy: 0.8846 - val_loss: 0.9793\n",
            "Epoch 9/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9302 - loss: 0.9282 - val_accuracy: 0.8846 - val_loss: 0.9536\n",
            "Epoch 10/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9190 - loss: 0.8987 - val_accuracy: 0.9231 - val_loss: 0.9301\n",
            "Epoch 11/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9317 - loss: 0.8814 - val_accuracy: 0.9231 - val_loss: 0.8988\n",
            "Epoch 12/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9285 - loss: 0.8381 - val_accuracy: 0.9231 - val_loss: 0.8727\n",
            "Epoch 13/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9390 - loss: 0.7951 - val_accuracy: 0.9231 - val_loss: 0.8467\n",
            "Epoch 14/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9431 - loss: 0.7514 - val_accuracy: 0.9231 - val_loss: 0.8119\n",
            "Epoch 15/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8942 - loss: 0.7645 - val_accuracy: 0.9231 - val_loss: 0.7775\n",
            "Epoch 16/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9265 - loss: 0.7010 - val_accuracy: 0.9231 - val_loss: 0.7483\n",
            "Epoch 17/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9087 - loss: 0.6784 - val_accuracy: 0.9231 - val_loss: 0.7193\n",
            "Epoch 18/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9119 - loss: 0.6566 - val_accuracy: 0.9231 - val_loss: 0.6878\n",
            "Epoch 19/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9244 - loss: 0.5912 - val_accuracy: 0.9231 - val_loss: 0.6556\n",
            "Epoch 20/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9244 - loss: 0.6046 - val_accuracy: 0.9231 - val_loss: 0.6261\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9175 - loss: 0.6237 \n",
            "Test Accuracy: 92.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "web cam"
      ],
      "metadata": {
        "id": "E638wmd2Mu8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Real-time activity recognition with webcam\n",
        "yolo_model = YOLO(\"yolo11n-pose.pt\")\n",
        "classifier_model = model\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "output_video_path = '/content/drive/MyDrive/P2/YOLO/output/activity_recognition_output.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (frame_width, frame_height))\n",
        "\n",
        "keypoints_data = []\n",
        "activity_labels = {0: 'sitting', 1: 'laying', 2: 'jumping'}\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    results = yolo_model(frame, verbose=False)\n",
        "\n",
        "    keypoints_list = []\n",
        "    for r in results:\n",
        "        keypoints = r.keypoints.xyn.tolist()\n",
        "        for keypoints_per_person in keypoints:\n",
        "            keypoints_flat = [coord for kp in keypoints_per_person for coord in kp]\n",
        "            keypoints_list.append(keypoints_flat)\n",
        "\n",
        "    if keypoints_list:\n",
        "        keypoints_array = np.array(keypoints_list)\n",
        "        predictions = classifier_model.predict(keypoints_array)\n",
        "        predicted_activity = np.argmax(predictions, axis=1)\n",
        "\n",
        "        for i, activity in enumerate(predicted_activity):\n",
        "            label = activity_labels[activity]\n",
        "            print(f\"Person {i+1} activity: {label}\")\n",
        "\n",
        "            keypoints = keypoints_list[i]\n",
        "            x1, y1 = int(keypoints[0] * frame_width), int(keypoints[1] * frame_height)\n",
        "            x2, y2 = int(keypoints[6] * frame_width), int(keypoints[7] * frame_height)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "            data = {'frame': len(keypoints_data) + 1, 'activity': label}\n",
        "            for j in range(0, len(keypoints), 2):\n",
        "                data[f'x{j//2}'] = keypoints[j]\n",
        "                data[f'y{j//2}'] = keypoints[j + 1]\n",
        "            keypoints_data.append(data)\n",
        "\n",
        "    out.write(frame)\n",
        "    cv2.imshow('Activity Recognition', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Save real-time keypoints data to CSV\n",
        "df_keypoints = pd.DataFrame(keypoints_data)\n",
        "keypoints_csv_path = '/content/drive/MyDrive/P2/DATA/webcam_keypoints.csv'\n",
        "df_keypoints.to_csv(keypoints_csv_path, index=False)\n",
        "\n",
        "print(f\"Video saved to {output_video_path}\")\n",
        "print(f\"Keypoints data saved to {keypoints_csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBY1XoZ6MueE",
        "outputId": "7ca0877e-7f24-4a10-fddf-296c3dde6bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved to /content/drive/MyDrive/P2/YOLO/output/activity_recognition_output.mp4\n",
            "Keypoints data saved to /content/drive/MyDrive/P2/DATA/webcam_keypoints.csv\n"
          ]
        }
      ]
    }
  ]
}